{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:27:55.497021Z",
     "start_time": "2021-05-10T20:27:55.474083Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import axes3d  \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import rcca\n",
    "from EDA_helper import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:27:56.116420Z",
     "start_time": "2021-05-10T20:27:56.074478Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_rf(stratified=False):\n",
    "    \"\"\" \n",
    "    1. Read in energ, ratio\n",
    "\n",
    "    2. format_df(ratio, ratings)\n",
    "\n",
    "    2b. format_df(energ, ratings)\n",
    "\n",
    "    merged = left_join(ratio, energ)\n",
    "\n",
    "    3. join_numerical(merged, ratings)\n",
    "\n",
    "    4. data_split to get holdout set (apply drop_na on this)\n",
    "\n",
    "    will have quarterly granularity for test\n",
    "\n",
    "    5. interpolate train data to fill in missing (expand quarterly to monthly) \n",
    "    \"\"\"\n",
    "    ratings = pd.read_excel(\"../datasets/ratings_all_energ_new.xlsx\")\n",
    "    ratio = pd.read_excel(\"../datasets/ratios_2_all_energ_new.xlsx\")\n",
    "    energ = pd.read_excel(\"../datasets/energ_specific_all_new.xlsx\")\n",
    "    # ratings = pd.read_excel(\"../datasets/ratings_2_06-17.xlsx\")\n",
    "    # ratio = pd.read_excel(\"../datasets/ratio_figs_2.xlsx\")\n",
    "    cols = ratio.columns\n",
    "    ratio = ratio.drop(cols[0], axis=1)\n",
    "    ratio = ratio.rename(columns = {'Public Date':'Data Date','EXCHANGE TICKER SYMBOL - HISTORICAL':'Ticker Symbol'})\n",
    "\n",
    "    ratio1, ratings1 = format_df(ratio.copy(), ratings.copy(), \n",
    "                                 ['Trailing P/E to Growth (PEG) ratio','Dividend Yield', \n",
    "                                  'Interest/Average Total Debt', 'Free Cash Flow/Operating Cash Flow'])\n",
    "\n",
    "    numer = list(ratio1.columns[4:-1])\n",
    "    # yr = ['2006', '2007', '2008','2009','2010','2011','2012','2013']\n",
    "    # X1,Y1,full1 = join_numerical(ratio1, ratings1, numerical=numer)\n",
    "    cols = energ.columns\n",
    "#     print(cols)\n",
    "    # aa = cols[[0,1,2,3,4,5,6, 18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]]\n",
    "    aa = cols[[0,1,2,3,4,5,6, 10,11,14,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]]\n",
    "    energ1 = energ.drop(\n",
    "           aa, \n",
    "        axis=1)\n",
    "    # energ1 = energ_.fillna(energ_.mean()) \n",
    "    # energ1, _ = format_df(energ.copy(), ratings.copy(), \n",
    "    #                             aa)\n",
    "    energ1['Data Date'] = energ1['Data Date'].astype(str)\n",
    "    energ1.shape\n",
    "    features = pd.merge(ratio1, energ1,how='left',on=[\"Data Date\", \"Ticker Symbol\"])\n",
    "#     len(features[\"Ticker Symbol\"].unique())\n",
    "    lst3 = [value for value in ratio[\"Ticker Symbol\"].unique() if value in energ[\"Ticker Symbol\"].unique()]\n",
    "    # ratio1[\"Ticker Symbol\"].unique()\n",
    "#     len(lst3)\n",
    "#     print(len(ratio[\"Ticker Symbol\"].unique()), len(energ[\"Ticker Symbol\"].unique()), \n",
    "#           len(ratio1[\"Ticker Symbol\"].unique()), len(energ1[\"Ticker Symbol\"].unique()))\n",
    "\n",
    "    first_col = features.pop(\"Ticker Symbol\") \n",
    "    ff = len(features.columns)\n",
    "    features.insert(ff, \"Ticker Symbol\", first_col)\n",
    "    numer = list(features.columns[4:-1])\n",
    "\n",
    "    X1,Y1,full1 = join_numerical(features, ratings1, numerical=numer)\n",
    "    \n",
    "    if stratified:\n",
    "        full1 = full1.reset_index()\n",
    "        comps = list(full1['Ticker Symbol'].unique())\n",
    "        labout = []\n",
    "        inout = []\n",
    "        v_labout = []\n",
    "        v_inout = []\n",
    "        t_labout = []\n",
    "        t_inout = []\n",
    "        for c in comps:\n",
    "            curr_in = full1.index[full1['Ticker Symbol'] == c]\n",
    "            indXd = X1[curr_in,:]\n",
    "            indYd = Y1[curr_in]\n",
    "            if np.random.binomial(1, 0.9):\n",
    "                if np.random.binomial(1, 0.8):\n",
    "                    inout.append(indXd)\n",
    "                    labout.append(indYd)\n",
    "                else:\n",
    "                    v_inout.append(indXd)\n",
    "                    v_labout.append(indYd)\n",
    "            else:\n",
    "                t_inout.append(indXd)\n",
    "                t_labout.append(indYd)\n",
    "        X1,Y1, X_val_, Y_val, X_hold_,Y_hold = (np.vstack(inout),np.vstack(labout), np.vstack(v_inout),\n",
    "                                               np.vstack(v_labout), np.vstack(t_inout), np.vstack(t_labout)) \n",
    "        ss = StandardScaler()\n",
    "        scl_X = ss.fit_transform(X1)\n",
    "        scl_X = np.nan_to_num(scl_X)\n",
    "        enc_Y_, _ = encode(Y1, 'full') \n",
    "\n",
    "        X_val = ss.transform(X_val_)\n",
    "        X_val = np.nan_to_num(X_val)\n",
    "        Y_val, _ = encode(Y_val, 'full')\n",
    "        \n",
    "        X_hold = ss.transform(X_hold_)\n",
    "        X_hold = np.nan_to_num(X_hold)\n",
    "\n",
    "        Y_hold, _ = encode(Y_hold, 'full')\n",
    "        minny = np.abs(np.min((np.min(enc_Y_), np.min(Y_val), np.min(Y_hold))))\n",
    "        enc_Y = enc_Y_ + minny\n",
    "        Y_val += minny\n",
    "        Y_hold += minny\n",
    "\n",
    "\n",
    "    else:\n",
    "        X_val, Y_val = np.array([]),np.array([])\n",
    "        X1, X_hold_,Y1, Y_hold = train_test_split(X1, Y1, test_size=0.2, random_state=100)\n",
    "        ss = StandardScaler()\n",
    "        scl_X = ss.fit_transform(X1)\n",
    "        scl_X = np.nan_to_num(scl_X)\n",
    "        enc_Y_, _ = encode(Y1, 'full') \n",
    "\n",
    "        X_hold = ss.transform(X_hold_)\n",
    "        X_hold = np.nan_to_num(X_hold)\n",
    "\n",
    "        Y_hold, _ = encode(Y_hold, 'full')\n",
    "        minny = np.abs(np.min((np.min(enc_Y_), np.min(Y_hold))))\n",
    "        enc_Y = enc_Y_ + minny\n",
    "        Y_hold += minny\n",
    "    print(scl_X.shape, X_val.shape, X_hold.shape)\n",
    "    return scl_X, enc_Y, X_val, Y_val, X_hold, Y_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:21.849896Z",
     "start_time": "2021-05-10T20:27:56.855524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Companies in Ratings:  335\n",
      "Unique Companies in Features:  253\n",
      "changed\n",
      "Intersection of Companies:  97\n",
      "(6375, 26) (0,) (1594, 26)\n"
     ]
    }
   ],
   "source": [
    "## NOTE: we do not want to separate a Val set as GDA function will do that\n",
    "X_train, Y_train, X_val,Y_val, X_test, Y_test = process_rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For convenience, I wrote a function that allows us to perform hyperparameter tuning and regularization over our 4 model classes: LDA, QDA, LDA with SMOTE, QDA with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:28.801298Z",
     "start_time": "2021-05-10T20:28:28.775396Z"
    }
   },
   "outputs": [],
   "source": [
    "def GDA(X, Y, full=None, shrink=None, reg=0.0, smote=True):\n",
    "    \n",
    "    results = {}\n",
    "    if smote:\n",
    "        X_train, Y_train, _, _, X_test, Y_test = data_split(X, Y, full, split=[0.8, 0.0],smote=True, neigh=4)\n",
    "        # LDA w/ smote\n",
    "        lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage=shrink)\n",
    "        lda.fit(X_train, Y_train)\n",
    "        preds_train = lda.predict(X_train)\n",
    "        preds_test = lda.predict(X_test)\n",
    "        results['lda_smote'] = (np.mean(Y_train == preds_train), np.mean(Y_test == preds_test))\n",
    "        # QDA w/ smote\n",
    "        qda = QuadraticDiscriminantAnalysis(reg_param = reg)\n",
    "        qda.fit(X_train, Y_train)\n",
    "        preds_train = qda.predict(X_train)\n",
    "        preds_test = qda.predict(X_test)\n",
    "\n",
    "        results['qda_smote'] = (np.mean(Y_train == preds_train), np.mean(Y_test == preds_test))\n",
    "    \n",
    "    X_train, Y_train, _, _, X_test, Y_test = data_split(X, Y, full, split=[0.8, 0.0], smote=False)\n",
    "    # LDA w/o smote\n",
    "    lda = LinearDiscriminantAnalysis(solver='eigen',shrinkage=shrink)\n",
    "    lda.fit(X_train, Y_train)\n",
    "    preds_train = lda.predict(X_train)\n",
    "    preds_test = lda.predict(X_test)\n",
    "    results['lda'] = (np.mean(Y_train == preds_train), np.mean(Y_test == preds_test))\n",
    "    # QDA w/o smote\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param = reg)\n",
    "    qda.fit(X_train, Y_train)\n",
    "    preds_train = qda.predict(X_train)\n",
    "    preds_test = qda.predict(X_test)\n",
    "    results['qda'] = (np.mean(Y_train == preds_train), np.mean(Y_test == preds_test))    \n",
    "    \n",
    "    df_ = pd.DataFrame(results.values(),index=results.keys(), columns = ['train accuracy, reg: ' + str(reg), 'test accuracy, reg: ' + str(reg)])\n",
    "    max_test=np.argmax(df_.iloc[:, 1:].values)\n",
    "    max_elem = pd.DataFrame(df_.iloc[max_test, :])\n",
    "    return df_, max_elem\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LDA/QDA assumptions\n",
    "\n",
    "Before we can proceed with applying these models, we must first see if they satisfy the necessary assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA assumptions\n",
    " - Class conditional distributions are Gaussian: $p(\\textbf{X} \\mid Y = k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$\n",
    " - All class conditional dist. have equal covariance: $\\forall k$ classes &nbsp; &nbsp; $\\Sigma_k = \\Sigma$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### QDA assumptions\n",
    " - Class conditional distributions are Gaussian: $p(\\textbf{X} \\mid Y = k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$\n",
    " - Conditional dist. can have unequal covariance: $\\Sigma_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:30.323527Z",
     "start_time": "2021-05-10T20:28:30.063806Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HZResults(hz=80, pval=0.0, normal=False)\n",
      "HZResults(hz=108, pval=0.0, normal=False)\n",
      "HZResults(hz=4.560702577813036, pval=0.0, normal=False)\n",
      "HZResults(hz=16.355319199558846, pval=0.0, normal=False)\n",
      "HZResults(hz=8.17824057260861, pval=0.0, normal=False)\n",
      "HZResults(hz=5.352710485729784, pval=0.0, normal=False)\n",
      "HZResults(hz=5.718241597681815, pval=0.0, normal=False)\n",
      "HZResults(hz=5.184256784855533, pval=0.0, normal=False)\n",
      "HZResults(hz=4.108393757576158, pval=0.0, normal=False)\n",
      "HZResults(hz=11.415270564692657, pval=0.0, normal=False)\n",
      "HZResults(hz=6.219344406953118, pval=0.0, normal=False)\n",
      "HZResults(hz=3.216211650341139, pval=0.0, normal=False)\n",
      "HZResults(hz=4.121419631253522, pval=0.0, normal=False)\n",
      "HZResults(hz=76, pval=0.0, normal=False)\n",
      "HZResults(hz=32, pval=0.0, normal=False)\n",
      "HZResults(hz=412, pval=0.0, normal=False)\n",
      "HZResults(hz=32, pval=0.0, normal=False)\n",
      "HZResults(hz=400, pval=0.0, normal=False)\n"
     ]
    }
   ],
   "source": [
    "from pingouin import multivariate_normality\n",
    "# Henze-Zirkler Multivariate Normality Test\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "for k in classes:\n",
    "    sub_x = X_train[Y_train == k,:]\n",
    "    print(multivariate_normality(sub_x, alpha=.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the p-values for our statistic test are below 0.5, we reject the null hypothesis and conclude that $p(\\textbf{X} \\mid Y = k)$ are not Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a quick sanity check, we plot the class conditional distributions for some class k over a few features. Clearly we see the data is NOT Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:31.831428Z",
     "start_time": "2021-05-10T20:28:31.639003Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sanity_x = X_train[Y_train == classes[len(classes)//2],:]\n",
    "pd.Series(sanity_x[0]).hist()\n",
    "pd.Series(sanity_x[3]).hist()\n",
    "pd.Series(sanity_x[6]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for equal Covariance\n",
    "\n",
    "Inspecting only at the variances of the class conditional covariance matrices, we see that they don't match up at all over different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:32.747982Z",
     "start_time": "2021-05-10T20:28:32.704926Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.95704749e-02, 1.56167284e-03, 2.36260222e-02, 1.02447729e-02,\n",
       "        4.12242690e-02, 5.63990539e-03, 2.78329718e-01, 9.68363344e-01,\n",
       "        2.71858140e-01, 2.42842277e+00, 1.78129330e-01, 5.05709368e-01,\n",
       "        1.50915772e-02, 4.75508528e-01, 1.76288468e-02, 1.25694937e-01,\n",
       "        2.34695861e-01, 2.80472477e-02, 1.66301036e-01, 2.01842991e-01,\n",
       "        2.81512124e-01, 1.41572630e-02, 1.31241659e-02, 6.57737050e-03,\n",
       "        2.28066656e-02, 3.39824330e-02],\n",
       "       [1.95954772e-01, 1.94786596e-04, 6.36289148e+00, 8.55611813e-03,\n",
       "        3.16979862e-02, 9.00480918e-03, 6.14856388e+01, 1.95899780e-01,\n",
       "        1.35738726e+00, 2.46953595e+00, 5.12555816e-01, 4.27489674e-01,\n",
       "        3.02977487e-03, 5.01742121e-01, 7.34316085e-01, 3.24445855e-01,\n",
       "        2.78598323e-01, 5.00492106e+00, 1.81453177e-01, 2.30951517e-01,\n",
       "        4.11492244e-01, 1.87126053e-02, 2.66451383e-02, 1.48674444e-02,\n",
       "        0.00000000e+00, 2.55797008e-02],\n",
       "       [5.12970429e-01, 5.05336166e-01, 2.08163242e+00, 6.54739875e-03,\n",
       "        2.40240672e-02, 5.97015588e-03, 9.19723171e-01, 8.05428530e-01,\n",
       "        2.13390608e+00, 2.01600871e+00, 1.72485791e+00, 7.09429324e-01,\n",
       "        4.79718835e-03, 1.29238969e+00, 1.35744039e-01, 2.88582866e-01,\n",
       "        6.22832644e-01, 1.24048376e+00, 1.89112480e-02, 2.70378665e-01,\n",
       "        1.62306608e-01, 7.17441788e-03, 1.63295446e-02, 9.52258594e-03,\n",
       "        1.06827415e-02, 1.81858765e-02],\n",
       "       [3.49355168e-01, 1.86417970e+00, 8.59747636e-01, 6.74027078e+00,\n",
       "        6.69480293e+00, 6.74885206e+00, 7.07734175e-01, 1.62893858e+00,\n",
       "        4.91648581e-01, 1.79453833e+00, 1.05825528e+00, 1.12260474e+00,\n",
       "        8.06408782e-03, 1.08851000e+00, 6.52154612e-01, 4.50293589e+00,\n",
       "        3.64979627e+00, 2.58283751e+00, 9.25491021e-02, 2.45985843e-01,\n",
       "        2.27540263e-01, 1.49531629e-02, 2.20823782e-02, 1.18609474e-02,\n",
       "        7.32452807e-03, 3.04385951e-02],\n",
       "       [7.26009482e-01, 1.79332211e+00, 3.58509795e+00, 5.34771400e-03,\n",
       "        1.41650203e-02, 5.31362957e-03, 5.52223208e-01, 6.73800193e-01,\n",
       "        1.73389180e+00, 1.48775297e+00, 1.94494956e+00, 8.65146643e-01,\n",
       "        6.29448542e-03, 1.00677337e+00, 7.07738676e+00, 1.12941187e+00,\n",
       "        1.04176740e+00, 4.16115870e+00, 8.29332251e-02, 1.97076466e-01,\n",
       "        1.13837391e-01, 7.36438226e-03, 1.41548043e-02, 6.81234520e-03,\n",
       "        6.43815804e-03, 2.18571019e-02],\n",
       "       [4.87400100e-01, 1.35732745e+00, 1.26211793e-01, 1.31790208e-03,\n",
       "        2.28725353e-03, 1.32377969e-03, 5.16972988e-01, 8.51921156e-01,\n",
       "        2.96019028e+00, 6.02259304e-01, 9.88880490e-01, 8.18757764e-01,\n",
       "        2.19905751e-02, 7.28154012e-01, 2.73401352e+00, 5.61379135e-01,\n",
       "        1.04353485e+00, 2.86970593e-01, 4.02841389e-02, 2.10202620e-01,\n",
       "        1.11486134e-01, 2.32465077e-03, 2.05644245e-02, 5.25751740e-03,\n",
       "        2.37796844e-02, 3.28892265e-02],\n",
       "       [5.15944236e+00, 2.32448525e+00, 1.21812866e+00, 9.33349742e-04,\n",
       "        3.93301991e-03, 8.74888724e-04, 4.58281887e-01, 1.94725714e+00,\n",
       "        1.47636945e-01, 8.61032787e-01, 1.12815327e+00, 7.99280693e-01,\n",
       "        6.59502328e-02, 7.93177067e-01, 1.89268708e-02, 2.72126455e-01,\n",
       "        4.60338433e-01, 2.92823983e-01, 6.56229299e-02, 1.35828371e-01,\n",
       "        7.89019169e-02, 6.09694743e-03, 1.45842662e-02, 7.56329144e-03,\n",
       "        2.14961427e-02, 2.86208017e-02],\n",
       "       [1.00893766e-01, 7.27972975e-01, 5.67000903e-02, 8.79607267e-03,\n",
       "        5.14701786e-02, 9.36471551e-03, 4.44588038e-01, 7.49831029e-01,\n",
       "        1.65413096e-01, 9.51963133e-01, 7.83648325e-01, 1.71298335e+00,\n",
       "        9.87676197e+00, 9.01059222e-01, 3.57979197e-01, 5.15278460e-01,\n",
       "        5.80015451e-01, 2.92231112e-01, 8.63280799e-02, 1.57506667e-01,\n",
       "        1.21838668e-01, 7.94268206e-03, 1.62379857e-02, 5.98250566e-03,\n",
       "        2.30809900e-02, 3.24023077e-02],\n",
       "       [1.28829830e+00, 2.50167470e-01, 5.28131053e-02, 1.57292209e-03,\n",
       "        8.65269627e-03, 1.26213950e-03, 4.36049763e-01, 8.24679836e-01,\n",
       "        3.65163777e+00, 1.06434905e+00, 5.48714218e-01, 6.04155868e-01,\n",
       "        3.98169659e-01, 4.41173864e-01, 5.74828177e-03, 2.76286705e-01,\n",
       "        2.86153752e-01, 9.84633293e-02, 2.30829474e-01, 1.39359501e-01,\n",
       "        2.96486047e-01, 2.69667303e-02, 1.35619441e-01, 4.07419621e-02,\n",
       "        8.17916659e-02, 5.05952120e-02],\n",
       "       [5.64084122e-01, 2.51976246e-01, 3.30421593e-01, 8.82655387e-04,\n",
       "        3.18765363e-03, 7.41871410e-04, 5.65832257e-01, 2.40703631e-01,\n",
       "        1.00360671e+00, 4.25369586e-01, 4.42310206e-01, 6.24592913e-01,\n",
       "        2.71064649e-02, 7.23820590e-01, 1.67764785e-02, 1.82944261e-01,\n",
       "        1.71758007e-01, 1.42901951e-01, 8.15841143e-02, 6.25494545e-02,\n",
       "        1.24327542e-01, 1.84369304e-01, 4.51224668e-02, 2.70557397e-02,\n",
       "        2.23086171e-01, 8.78601390e-02],\n",
       "       [5.70072008e-02, 3.09401028e-01, 2.42760509e+00, 6.70921069e-04,\n",
       "        2.85493859e-03, 5.53961908e-04, 1.67979432e-01, 7.60873285e-01,\n",
       "        3.04890852e-02, 5.74010371e-01, 1.05889689e+00, 1.11376503e+00,\n",
       "        6.75172460e-02, 3.05964306e-01, 2.53703225e-03, 2.59728152e-01,\n",
       "        3.74740146e-01, 3.60792006e-01, 6.18804328e-02, 7.04549576e-02,\n",
       "        8.95436303e-02, 1.70008203e-01, 3.24424901e-02, 2.55473619e-02,\n",
       "        1.36110335e-02, 5.79660302e-03],\n",
       "       [1.42143457e-02, 4.75331354e-02, 2.65995527e-01, 3.28825050e-04,\n",
       "        2.29099603e-03, 4.82890686e-04, 4.62884626e-01, 6.66125690e-01,\n",
       "        7.84550021e-03, 5.77510856e-01, 1.05882598e+00, 1.12195431e+00,\n",
       "        7.29553855e-02, 5.39351694e-01, 1.47405335e-02, 7.45687746e-02,\n",
       "        1.24219744e-01, 2.16314374e-01, 1.46569385e-01, 1.34449977e-01,\n",
       "        2.54113582e-01, 7.62627835e-02, 3.18020355e-01, 2.52663807e-01,\n",
       "        4.38895110e-02, 3.48316669e-02],\n",
       "       [1.99859901e-02, 4.52683897e-02, 2.18273270e-01, 3.29810987e-04,\n",
       "        1.19873877e-03, 2.95836826e-04, 1.65998616e-01, 6.83557132e-01,\n",
       "        4.38621297e-03, 1.50300627e-01, 2.27211498e-01, 8.75669956e-01,\n",
       "        1.71796197e-02, 2.55551074e-01, 1.38553071e-03, 1.22534459e-01,\n",
       "        4.03540036e-01, 1.42151851e-01, 1.16330712e-01, 1.29304432e-01,\n",
       "        1.05818035e-01, 2.40881096e-01, 1.24139407e+00, 1.49914649e+00,\n",
       "        2.33155748e-01, 6.02009109e-01],\n",
       "       [1.16842544e-04, 2.40286546e-03, 3.23198472e-03, 1.82724750e-05,\n",
       "        7.60631375e-06, 1.69334575e-05, 8.09758985e-03, 4.01584256e-02,\n",
       "        1.42131059e-04, 9.04132500e-04, 2.66752952e-02, 1.43071927e-01,\n",
       "        7.61114724e-04, 8.01687250e-03, 1.38372051e-05, 1.11959907e-03,\n",
       "        6.10853356e-03, 9.51190323e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [1.44983090e-03, 1.58253423e+00, 2.19560219e-02, 6.10288320e-06,\n",
       "        4.28424578e-06, 1.53374251e-06, 2.49583478e-03, 3.21614602e-03,\n",
       "        7.84236158e-04, 5.16173952e-03, 2.05140588e-02, 0.00000000e+00,\n",
       "        3.18831453e-07, 2.09608379e-03, 9.94888286e-07, 1.23761059e-03,\n",
       "        2.30753840e-03, 2.01192382e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.04930367e+00, 3.58859541e-01],\n",
       "       [2.69597211e-04, 2.77688071e-04, 2.07943161e-03, 2.65350444e-06,\n",
       "        1.70457109e-05, 1.13424626e-05, 2.09118637e-02, 8.54189261e-02,\n",
       "        1.59226745e-04, 2.15533543e-02, 4.87759650e-02, 1.58923290e-01,\n",
       "        6.92047094e-04, 3.93737388e-02, 1.70422971e-04, 1.81151606e-02,\n",
       "        2.42046019e-02, 1.25864831e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.54612808e+00, 9.93295121e-01],\n",
       "       [7.70642228e-04, 1.80997510e-03, 1.40687287e-02, 5.20955626e-07,\n",
       "        1.45396579e-06, 7.84502138e-09, 2.50243122e-03, 2.44600549e-05,\n",
       "        1.76802915e-04, 7.89843671e-04, 6.15198081e-03, 0.00000000e+00,\n",
       "        3.26100023e-07, 4.48299980e-04, 5.84788192e-08, 2.17147712e-05,\n",
       "        1.51044152e-05, 4.16970252e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.95282015e-01],\n",
       "       [3.05729203e-04, 3.31546567e-04, 3.71943668e-03, 2.55601625e-06,\n",
       "        1.94272261e-05, 1.39508160e-06, 1.55469109e-02, 7.67243339e-01,\n",
       "        9.79414621e-05, 3.72136634e-02, 4.42891675e-02, 5.25854719e-01,\n",
       "        5.39243337e-04, 1.87980852e-02, 5.71617333e-05, 8.43956385e-02,\n",
       "        8.77024097e-02, 3.60225075e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.02731920e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(Y_train)\n",
    "vars_ = np.zeros((classes.shape[0], X_train.shape[1]))\n",
    "for i,k in enumerate(classes):\n",
    "    sub_x = X_train[Y_train == k,:]\n",
    "    df_temp = pd.DataFrame(sub_x).cov()\n",
    "    vars_[i] = df_temp.to_numpy().diagonal() \n",
    "    \n",
    "vars_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We conclude that the assumptions for Gaussian Discriminant Analysis fail and thus we shouldn't expect great results from this method. However, we may still be able to get good results despite this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:28:35.881260Z",
     "start_time": "2021-05-10T20:28:33.605377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy:  0.4235294117647059\n",
      "Best Regularization:  1e-05\n",
      "Best Model:  qda_smote\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy, reg: 0.0</th>\n",
       "      <th>test accuracy, reg: 0.0</th>\n",
       "      <th>train accuracy, reg: 1e-05</th>\n",
       "      <th>test accuracy, reg: 1e-05</th>\n",
       "      <th>train accuracy, reg: 0.0001</th>\n",
       "      <th>test accuracy, reg: 0.0001</th>\n",
       "      <th>train accuracy, reg: 0.001</th>\n",
       "      <th>test accuracy, reg: 0.001</th>\n",
       "      <th>train accuracy, reg: 0.01</th>\n",
       "      <th>test accuracy, reg: 0.01</th>\n",
       "      <th>train accuracy, reg: 0.1</th>\n",
       "      <th>test accuracy, reg: 0.1</th>\n",
       "      <th>train accuracy, reg: 1.0</th>\n",
       "      <th>test accuracy, reg: 1.0</th>\n",
       "      <th>train accuracy, reg: 10.0</th>\n",
       "      <th>test accuracy, reg: 10.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda_smote</th>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.312941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda_smote</th>\n",
       "      <td>0.535524</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.536664</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.526026</td>\n",
       "      <td>0.420392</td>\n",
       "      <td>0.503609</td>\n",
       "      <td>0.401569</td>\n",
       "      <td>0.460866</td>\n",
       "      <td>0.371765</td>\n",
       "      <td>0.381459</td>\n",
       "      <td>0.305098</td>\n",
       "      <td>0.272986</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.004706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "      <td>0.324902</td>\n",
       "      <td>0.304314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>0.435686</td>\n",
       "      <td>0.418039</td>\n",
       "      <td>0.435490</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.418627</td>\n",
       "      <td>0.398431</td>\n",
       "      <td>0.387647</td>\n",
       "      <td>0.369412</td>\n",
       "      <td>0.358431</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.300980</td>\n",
       "      <td>0.303529</td>\n",
       "      <td>0.259804</td>\n",
       "      <td>0.241569</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.004706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           train accuracy, reg: 0.0  test accuracy, reg: 0.0  \\\n",
       "lda_smote                  0.331687                 0.312941   \n",
       "qda_smote                  0.535524                 0.419608   \n",
       "lda                        0.324902                 0.304314   \n",
       "qda                        0.435686                 0.418039   \n",
       "\n",
       "           train accuracy, reg: 1e-05  test accuracy, reg: 1e-05  \\\n",
       "lda_smote                    0.331687                   0.312941   \n",
       "qda_smote                    0.536664                   0.423529   \n",
       "lda                          0.324902                   0.304314   \n",
       "qda                          0.435490                   0.414902   \n",
       "\n",
       "           train accuracy, reg: 0.0001  test accuracy, reg: 0.0001  \\\n",
       "lda_smote                     0.331687                    0.312941   \n",
       "qda_smote                     0.526026                    0.420392   \n",
       "lda                           0.324902                    0.304314   \n",
       "qda                           0.418627                    0.398431   \n",
       "\n",
       "           train accuracy, reg: 0.001  test accuracy, reg: 0.001  \\\n",
       "lda_smote                    0.331687                   0.312941   \n",
       "qda_smote                    0.503609                   0.401569   \n",
       "lda                          0.324902                   0.304314   \n",
       "qda                          0.387647                   0.369412   \n",
       "\n",
       "           train accuracy, reg: 0.01  test accuracy, reg: 0.01  \\\n",
       "lda_smote                   0.331687                  0.312941   \n",
       "qda_smote                   0.460866                  0.371765   \n",
       "lda                         0.324902                  0.304314   \n",
       "qda                         0.358431                  0.352941   \n",
       "\n",
       "           train accuracy, reg: 0.1  test accuracy, reg: 0.1  \\\n",
       "lda_smote                  0.331687                 0.312941   \n",
       "qda_smote                  0.381459                 0.305098   \n",
       "lda                        0.324902                 0.304314   \n",
       "qda                        0.300980                 0.303529   \n",
       "\n",
       "           train accuracy, reg: 1.0  test accuracy, reg: 1.0  \\\n",
       "lda_smote                  0.331687                 0.312941   \n",
       "qda_smote                  0.272986                 0.253333   \n",
       "lda                        0.324902                 0.304314   \n",
       "qda                        0.259804                 0.241569   \n",
       "\n",
       "           train accuracy, reg: 10.0  test accuracy, reg: 10.0  \n",
       "lda_smote                   0.331687                  0.312941  \n",
       "qda_smote                   0.016717                  0.004706  \n",
       "lda                         0.324902                  0.304314  \n",
       "qda                         0.002745                  0.004706  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix,mean_absolute_error, classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "dfs = []\n",
    "best_reg = 0\n",
    "best_test = 0\n",
    "best_model = ''\n",
    "# for j in range(1,len(xs)):\n",
    "for i in [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    df, maxi = GDA(X_train, Y_train, shrink='auto', reg=i)\n",
    "    if maxi.iloc[1, 0] > best_test:\n",
    "        best_reg = i\n",
    "        best_test = maxi.iloc[1, 0]\n",
    "        best_model = maxi.columns[0]\n",
    "#         best_data = j\n",
    "    dfs.append(df)\n",
    "    \n",
    "#     print(maxi)\n",
    "print(\"Best Test Accuracy: \",best_test)\n",
    "print(\"Best Regularization: \",best_reg)\n",
    "print(\"Best Model: \", best_model)\n",
    "# print(mean_absolute_error())\n",
    "# print('Best Data: ', best_data)\n",
    "pd.concat(dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, Gaussian Discriminant Analysis doesn't perform anywhere near the desired level compared to other baselines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
